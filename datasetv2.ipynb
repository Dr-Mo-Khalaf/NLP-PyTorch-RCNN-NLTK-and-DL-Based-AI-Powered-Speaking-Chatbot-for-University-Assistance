{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52509b81-f712-430e-8b0b-a138b88442da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "# from nltk_utils import tokenize_fn, stem_fn, lemmatize_fn ,bag_of_words\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from classes import createChatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a211500-a548-4a3b-896f-5c45c0805b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\University\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\University\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\University\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define stop words using nltk's stopword list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# The ignore list containing words to exclude\n",
    "ignore_list = [\"\"]\n",
    "\n",
    "# Function to clean contractions\n",
    "def clean_contractions(word):\n",
    "    return re.sub(r\"['â€™]s$\", \"\", word).replace(\"n't\", \"\")\n",
    "\n",
    "# Function to tokenize and lemmatize while filtering\n",
    "\n",
    "def preprocess_words_fn(pattern, stop_words, punctuation):\n",
    "    \"\"\"\n",
    "    Preprocess a sentence or a list of words:\n",
    "    - Tokenizes if it's a string.\n",
    "    - Filters stopwords and punctuation.\n",
    "    - Lemmatizes the words.\n",
    "    \"\"\"\n",
    "    all_word=[]\n",
    "    if isinstance(pattern, str):  # Check if input is a string\n",
    "        words = nltk.word_tokenize(pattern.lower())  # Tokenize and convert to lowercase\n",
    "    elif isinstance(pattern, list):  # If it's already a list\n",
    "        words = [word.lower() for word in pattern]  # Normalize case\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a string or a list of words.\")\n",
    "\n",
    "    words = [clean_contractions(w) for w in words]  # Clean contractions like 's and n't\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]  # Lemmatize words\n",
    "    words = [w for w in words if w not in stop_words and w not in punctuation]  # Filter\n",
    "    words = [ w for w in words if len(w)>1]\n",
    "    # print(\"---<\" , words)\n",
    "    all_word.extend(words)\n",
    "    # print(\"\\n \\n \\n <<>>>>>\" , len(all_words))\n",
    "    # print(\"\\n \\n \\n <<>>>>>\" , len(sorted(set(all_words))))\n",
    "    return sorted(set(all_word))  # Return unique words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23927b46-5d18-4f1e-9fda-30a1f05eb282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1716b51f-8561-4de8-bef4-9bc5935f2e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents2.json', \"r\") as f:\n",
    "    all_intents = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1143432e-0043-4f8e-9076-1d6965fb6719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "# print(all_intents)\n",
    "for intent in all_intents[\"intents\"] :\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        w = preprocess_words_fn(pattern=pattern , stop_words= stop_words ,\n",
    "                             punctuation=punctuation )\n",
    "        all_words.extend(w)\n",
    "        xy.append((w,tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cd343e5-66e3-42b8-91aa-f33ea9ed84b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154 patterns \n",
      "\n",
      "7 tags:\n",
      " ['greeting', 'goodbye', 'practical_programs', 'study_AI', 'study_tips', 'best_study_time', 'student_support']\n",
      "495 all words:\n",
      "  all_words\n"
     ]
    }
   ],
   "source": [
    "print(len(xy) , \"patterns \\n\" )\n",
    "print(len(tags), \"tags:\\n\", tags)\n",
    "print(len(all_words),\"all words:\\n\",\" all_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2892813d-88a2-4dfa-a3fd-d7f08113b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def bag_of_words(sentence, all_words, preprocess_words_fn, stop_words, punctuation):\n",
    "    \"\"\"\n",
    "    Return a bag of words: 1 for each known word that exists in the sentence, 0 otherwise.\n",
    "    sentence = \"hello how are you\"\n",
    "    all_words = [\"hi\", \"hello\", \"I\", \"bye\", \"thank\", \"cool\"]\n",
    "    bag = [0, 1, 0, 0, 0, 0]\n",
    "    \"\"\"\n",
    "    # Preprocess the sentence (e.g., tokenization, lemmatization)\n",
    "    sentence_words = preprocess_words_fn(sentence, stop_words, punctuation)\n",
    "\n",
    "    # Initialize the bag with 0s for each word in all_words|\n",
    "    bag = np.zeros(len(all_words), dtype=np.float32)\n",
    "    # print(bag)\n",
    "    # Loop through each word in all_words\n",
    "    for idx, word in enumerate(all_words):\n",
    "        if word in sentence_words:  # If the word exists in the sentence\n",
    "            bag[idx] = 1  # Mark it as present\n",
    "\n",
    "    return bag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a5104cb-4893-49d3-9ef1-da7688f113f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bag of the words \n",
    "# create training data\n",
    "X_train = []\n",
    "y_train = [] \n",
    "\n",
    "for (pattern_sentence , tag) in xy:\n",
    "    # print(pattern_sentence)\n",
    "    bag =  bag_of_words(pattern_sentence, all_words ,preprocess_words_fn, \n",
    "                        stop_words, punctuation)\n",
    "    # print(\"bag\" , bag)\n",
    "    X_train.append(bag)\n",
    "\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label) # for calculate the loss_fn -- CreossEntropy\n",
    "    classes = tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ac1076e-c3f5-43e8-93ba-82c313f99703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example data\n",
    "# pat = \"How can I manage my time effectively?\"\n",
    "    \n",
    "\n",
    "# # Preprocess words\n",
    "# allword = preprocess_words(pat, stop_words, punctuation)\n",
    "\n",
    "# # Output the processed words\n",
    "# print(allword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be6df9-24a9-4128-a839-68b4a7188d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37f90327-9866-4329-9130-482cfcc12c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 1., 1., 1.]], dtype=float32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_train,  y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4780ebc-1c7f-4c6a-833f-f715383bb98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    }
   ],
   "source": [
    "train_dataset = createChatDataset(X_train,y_train,classes, all_words)\n",
    "\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc4a0ef4-672f-4cee-bf65-683c0a890b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greeting',\n",
       " 'goodbye',\n",
       " 'practical_programs',\n",
       " 'study_AI',\n",
       " 'study_tips',\n",
       " 'best_study_time',\n",
       " 'student_support']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4fbcc74-1388-44a1-a9fc-9d2c831e2579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['hello', 'hi', 'hey', 'good'], 495)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.all_words[:4] , len(train_dataset.all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81d16057-dceb-4ac9-9b10-c9dc2e1eb7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, \"train_dataset_v2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec90f25d-0a32-4ba6-9b50-cc9834cb0dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773a2a8e-6c71-49de-bcff-772066b4553b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad6b2e-91d2-4720-8136-0c9e09c76127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f4a6ed-df43-4700-8476-9abcf4b630ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
